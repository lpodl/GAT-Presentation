\label{chapter:introduction}

As machine learning models find their way in more and more fields of science, allowing them to work with graph data has been a research topic with increasing popularity in recent years. Graphs are commonly used across various fields such as social science (social networks), natural science (protein-protein interactions) and knowledge graphs \cite{zhou2021graph}. In order to allow for this unique data structure, special neural networks have been developed, called \textbf{graph neural networks (GNNs)}. With advancements in the field of deep learning, especially the invention and success of convolutional neural networks, the development of similar methods for graphs suggests itself. In both cases, extracting localized features and composing them to more complex and expressive representations is a major step towards high-performance models for classification and clustering tasks. One way of generalizing convolutions in the graph domain are so-called spatial methods (\cite{hamilton2018inductive}, \cite{monti2016geometric}), where convolutions are defined directly on the graph, operating on groups of spatially close neighbors. Inspired by this idea, \cite{velickovic2018graph} proposed a new architecture to compute node representations by attending over a node's neighbors and assessing their individual importance, called \textbf{graph attention networks (GATs)}. This report covers a short explanation of the proposed idea and discusses some of its benefits in specific applications. Finally, we will give highlight related developments in the field, including concerns over prevalent data sets used for benchmarks and limitations of architectures like GATs.
\\~\\
